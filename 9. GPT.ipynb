{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNspoXaORFw7+6VeHsVvROC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1. GPT(Generative Pre-trained Transformer)\n","* GPT 모델은 2018년 6월에 OpenAI가 논문에서 처음 제안\n","* GPT도 unlabeled data로 부터 pre-train을 진행한 후, 이를 특정 downstream task(with labeled data)에 fine-tuning:미세 조정(transfer learning)을 하는 모델\n","* Transformer의 decoder만 사용하는 구조\n","* https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"],"metadata":{"id":"Ii6-uDzy1x_U"}},{"cell_type":"markdown","source":["**굵은 텍스트**### 1-1. GPT 모델의 특징\n","* 사전학습에는 대규모의 unlabeled data를 사용하는데 unlabeled data에서 단어 수준 이상의 정보를 얻는 것은 매우 힘듦 또한 어떤 방법이 유용한 텍스트 표현을 배우는데 효과적인지 불분명함\n","* 사전학습 이후에도, 어떤 방법이 fine-tuning에 가장 효과적인지 불분명\n","* 논문에서는 unsupervised pre-training과 supervised fine-tuning의 조합을 사용한 접근법을 제안\n","* 목표는 fine-tuning에서의 미세 조정만으로 다양한 자연어 처리 작업에 적용할 수 있는 범용적인 표현을 사전학습에서 학습하는 것\n","* 모델은 이미 효과가 검즌된 2017년 공개된 transformer를 사용"],"metadata":{"id":"ss6JjaD-3Gwf"}},{"cell_type":"markdown","source":["### 1-2. GPT 모델 구조\n","* GPT는 Transformer의 변형인 multi-layer Transformer decoder만 사용\n","* 입력 문맥 token에 multi-headed self-attention을 적용한 후, 목표 token에 대한 출력 분포를 얻기 위해 position-wise feedforward layer를 적용\n"],"metadata":{"id":"QyZHUAv-49PK"}},{"cell_type":"markdown","source":["### 1-3. GPT 모델 학습\n","* unsupervised pre-training: 대규모 코퍼스에서 unsupervised learning으로 던어 모델을 학습\n","  - transformer 디코더를 사용하여 계속 index token prediction 학습하는 것\n","  - nulti-layer Transformer decoder를 사용하여 입력 문맥 token에 multi-headed self-attention을 적용한 후 목표 token에 대한 출력 분포를 얻기 위해 position-wise feedforward layer를 적용\n","\n","* supervised fine-tuning:특정 작업에 대한 데이터로 모델을 fine-tuning\n","  - 파인튜닝 단계에서는 사전학습된 모델을 각 테스트에 맞게 input과 label로 구성된 supervised dataset에 대해 학습\n","  - GPT-1의 output을 downstream 태스트에 적절하도록 선형변환한 후 softmax에 넣음\n","  - 결과를 태스크에 맞는 loss들을 결합\n","\n","> 사전 학습은 next token prediction이라는 language modeling으로 진행되었기 때문에 각 downstream task와 input의 모양이 다를 수 밖에 없음\n","\n"],"metadata":{"id":"iF3efYfe5U92"}},{"cell_type":"markdown","source":["### 1-4. GPT 모델의 의의\n","* 모델 구조의 변형이 거의 없음\n","  - 이전의 사전학습 모델들은 finetune시 모델 구조를 변형해야 하는 문제점이 있음\n","  - 단, GPT-1은 GPT-1의 모델 구조를 전혀 안 바꾸고 입력만 바꿈\n","* 추가되는 파라미터의 수가 매우 적음\n","  - 모델 구조를 변형하지 않고,  Linear layer를 마지막에 추가하는 아주 간단한 추가작업만 수행\n","  - fintuning 시점에서 random하게 초기화된 initial weight가 거의 linear layer뿐!"],"metadata":{"id":"vPa5iIHG5vML"}},{"cell_type":"markdown","source":[],"metadata":{"id":"egFe19T-HGew"}}]}