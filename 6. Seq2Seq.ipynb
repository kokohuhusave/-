{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP/PdsnViqJuD7q52zxLFVd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1. 문장 임베딩\n","* 2017년 이전의 임베딩 기법들은 대부분 단어수준 모델이였음(Word2Vec, FastText, GloVe)\n","* 단어 수준 임베딩 기법은 자연어의 특성인 모호성, 동음이의어을 구분하기 어렵다는 한계가 있음\n","* 2017년도 이후에는 ELMO(Embeddings from Language Models)와 같은 모델이 발표되고 트렌스포머와 같은 언어 모델에서 문장 수준의 언어 모델링을 고려하면서 한계점들이 해결됨\n"],"metadata":{"id":"1-2UZLrse-4j"}},{"cell_type":"markdown","source":["###  1-1. 언어 모델\n","* 자연어처리 작업은 자연어 문장을 생성하거니 예측하는 방식으로 결과를 표현함\n","* 자연어처리 작업에서는 자연어를 수치화하여 표현할 수 있는 모델을 사용\n","* 언어 모델은 자연어 문장 혹은 단어에 확률을 할당하여 컴퓨터가 처리할 수 있도록 하는 모델로 주어진 입력에 대해 가장 자연스러운 단어 시퀀스를 찾을 수 있음\n"],"metadata":{"id":"W-JV9HBqgLN7"}},{"cell_type":"markdown","source":["### 1-2. 언어 모델링\n","* 주어진 단어들로부터 아직 모른는 단어들을 예측하는 작업\n","* 사람은 수많은 단어와 문장을 듣고 쓰고 말하면서 언어 능력을 학습해왔기 때문에 문장 구성 및 의미상 가장 적합한 단어를 판단할 수 있음\n","* 문장을 기계에게 보여주고 적합한 단어를 예측하도록 학습(기계 역시 사람과 닽은 프로세스로 동작함)\n","* 언어 모델은 텍스트 기반의 수많은 문장을 통해 어떤 단어가 어떤 어순으로 쓰인 것이 가장 자연스러운 문장인지 학습함\n","* 예시\n","\n","```\n","기차를 타기위해 기차역을 가는 중에 차가 너무 막혀서 결국 기차를 ___\n","1. 멈췄다\n","2. 기다렸다\n","3. 먹었다\n","4. 놓쳤다\n","5. 놓았아\n","```\n","\n"],"metadata":{"id":"XQIiF-zUgosb"}},{"cell_type":"markdown","source":["### 2. 자연어처리 모델 구조\n","* 자연어 처리 분야의 인공지능 모델은 근 10년 동안 수 많은 모델 구조에 걸쳐 진화해 왔음\n","* 사용되지 않는 모델들도 있으나, 해당 모델이 나온 이유와 구조, 한계점을 공부하는 것은 자연어 처리 분야에서 통찰을 얻는데 도움이 될 것\n","* 현제에도 분야별로 다양한 모델들이 공개되고 있는데 자연어처리에서 핵심은 공개된 모델들 중 어떤 모델이 내가 풀고자 하는 문제에 가장 적합한지 탐색하는 것\n","* 대부분의 분야에서 Transformer 계열의 모델이 가장 인기있지만, 특정 자연어 작업 처리에 특화된 세부적인 테크닉들이 다르므로 최신 연구 동향과 SOTA 모델들을 팔로업하는 좋음\n"],"metadata":{"id":"IyKv20_ajO98"}},{"cell_type":"markdown","source":["### 2-1. 자연어처리 분야의 주요 언어 모델\n","* Seq2Seq\n","* ELMO\n","* Transformer\n","* GPT\n","* BERT"],"metadata":{"id":"J9bEudD-hliz"}},{"cell_type":"markdown","source":["# 3. Seq2Seq 배경\n","* Seq2Seq 모델이 등장하기 전에 DNN(Deep Neural Network) 모델은 사물인식, 음성인식 등에서 꾸준히 성과를 냈음 (예: CNN, RNN, LSTM, GRU ..)\n","* 모델 입/출력의 크기가 고정된다는 한계점이 존재했기 때문에 자연어처리와 같은 가변적인 길이의 입/출력을 처리하는 문제들을 제대로 해결할 수 없었음\n","* [RNN](https://ardino-lab.com/rnn%EC%9D%98-%EA%B5%AC%EC%A1%B0-%EB%B0%8F-%ED%95%9C%EA%B3%84/)은 Seq2Seq기 등장하기 전에 입/출력을 기퀀스 단위로 처리할 수 있는 모델이었음\n","* RNN은 셀을 제귀적으로 활용하여 연속된 입/출력을 처리할 수 있는 모델\n","* 재귀호출: 자기자신을 다시 부르는것\n"],"metadata":{"id":"gKGh_vZGk1nn"}},{"cell_type":"markdown","source":["### 3-1. Seq2Seq(Sequence To Sequence)\n","* 2014년 구글에서 논문으로 제안한 모델\n","* LSTM(Long Short-Term Memory)또는 GRU(Gated Recurrent Unit)기반의 구조를 가지고 고정된 길이의 단어 기퀀스를 입력으로 받아, 입력 스퀀스에 알맞은 길이의 시퀀스를 출력해주는 언어 모델\n","* 본 논문은 2개의 LSTM을 각각 Encoder와 Decoder로 사용해 가변적인 길이의 입/출력을 처리하고자 했음\n","* [Seq2Seq](https://wikidocs.net/24996) 모델은 기계 번역 작업에서 큰 성능 향상을 가져왔고, 특히 긴 문장을 처리하는데 강점이 있음\n"],"metadata":{"id":"e-mf5AvRnjMz"}},{"cell_type":"markdown","source":["### 3-2. Seq2Seq 모델 구조\n","* Seq2Seq는 한 문장을 다른 문장으로 변환하는 모델\n","* 가변 길이의 입/출력을 처리하기 위해 인코더,디코더 구조를 채택\n","* 인코더와 디코더는 모두 여러개의 LSTM 또는 GRU셀로 구성되어 있음\n","* 바닐라 RNN 대신 LSTM과 GRU 셀을 사용하는 이유눈 LSTM과 GRU 모델이 RNN이 가지는 또 다른 한계점인 Long-term dependency를 해결하기 위해"],"metadata":{"id":"eVHe7FxLlmh5"}},{"cell_type":"markdown","source":["### 3-3.인코더\n","* 입력문장을 컨텍스트 벡터에 인코딩(압축)하는 역활을 함\n","* 인코더의 LSTM은 입력 문장을 단어 순서대로 처리하여 고정된 크기의 컨텍스트 벡터를 반환\n","* 컨텍스트 벡터는 인코더의 마지막 스텝에서 출력된 hidden state와 같음\n","* 컨텍스트 벡터는 입력 문장의 정보를 함축하는 벡터이므로, 해당 벡터를 입력 문장에 대한 문장 수준의 벡터로 활용할 수 있음\n"],"metadata":{"id":"QH1G4uIC3wnh"}},{"cell_type":"markdown","source":["### 3-4. 디코더\n","* 입력문장의 정보가 입축된 컨텍스트 벡터를 사용하여 출력 문장을 디코딩하는 역활\n","* 다코더의 LSTM은 인코더로부터 전달받은 컨켁스트 벡터와 문장을 시작을 뜻하는 <sos>토큰을 입력으로 받아서, 문장의 끝을 뜻하는 <eos토큰이 나올 때까지 문장을 생성\n","* LSTM의 첫 셀에서는  <sos>토큰과 컨텍스트 벡터를 입력받아서 그 다음에 등장할 확률이 가장 높은 단어를 에측하고, 다음 스텝애서 예측한 단어를 입력으로 받아서, 그 다음에 등장할 확률이 가장 높은 단어를 예측함\n","* 위 과정을 재귀적으로 반복하다가 다음에 등장할 확률이 가장 높은 단어로 <eos>토큰이 나오면 생성을 종료\n"],"metadata":{"id":"wvAM-JxN5HVj"}},{"cell_type":"markdown","source":["softmax를 통해 출력 단어를 예측"],"metadata":{"id":"0rHCKPji6IqP"}},{"cell_type":"markdown","source":["### 3-5. 학습 과정\n","* 모델을 학습할 때 Teacher Forcing이라는 기법을 사용\n","* 모델 학습 과정에서는 이전 셀에서 예측한 단어를 다음 셀의 입력으로 넣어주는 대신 실제 정답 단어를 다음 셀의 입력으로 넣음\n","* 만약 위 방법으로 학습하지 않으면 이전 셀에서의 오류가 다음 세롤 계속 전파될것이고, 그러면 학습이 제대로 되지 않음\n"],"metadata":{"id":"XEN16WWs2qK_"}},{"cell_type":"markdown","source":["### 3-6. Seq2Seq 모델의 한계점\n","* 가변적인 길이의 입/출력을 처리하는데 효과적인 모델 구조이며, 실제로 기계 번역작업에서 성능 향상을 거뒀으나 여전히 한계를 가짐\n","* 인코더가 출력하는 벡터 사이즈가 고정되어 있기 떄문에. 입력으로 들어오는 단어의 수가 매우 많아지면 성능이 떨어짐\n","* RNN 구조의 모델에서는 hidden state를 통해 이전 셀의 정보를 다음 셀로 계속 전달하는데 문자의 길이가 길어지면 초기 셀에서 전달했던 정보들이 점차 흐려짐\n","* LSTM, GRU 같은 모델들이 제안되긴 했으나 여전히 이전 정보를 계속 압축하는데 한계는 있음\n"],"metadata":{"id":"-1RB3WtW-zh1"}},{"cell_type":"markdown","source":["# 4. Attention meshanism\n","* 기본적으로 Seq2Seq 모델의 한계를 해결하기 위해 2014년도 제안\n","* 입력 시퀀스가 길어지면 출력 시퀀스의 정확도가 떨어지는 것을 보정해주기 위해 등장한 기법\n"],"metadata":{"id":"osZHSs5-_-U-"}},{"cell_type":"markdown","source":["### 4-1. 어텐션의 아이디어\n","* https://wikidocs.net/22893\n","* 디코더에서 출력단어를 예측하는 매 시점(time step)마다, 인코더에서의 전체 입력 문장을 다시 한 번 참고한다는 점\n","* 단, 전체 입력 문장을 전부 다 동일한 비율로 참고하는 것이 아니라, 해당 시점에서 예측해야 할 단어와 연관이 있는 입력 단어 부분을 좀 더 집중해서 보게 함"],"metadata":{"id":"EZGkNybTAbAx"}},{"cell_type":"markdown","source":["### 4-2. 어텐션 함수\n","* 어텐션을 함수로 표현\n","  - Attention(Q, K, V) = Attention Value\n","* 어텐션 함수는 주어진 쿼리에 대해서 모든 키와의 유사도를 각각 계산\n","* 계산된 유사도를 키와 맵핑되어 있는 각각의 값에 반영한 뒤 유사도가 반영된 값을 모두 더해서 반환(어텐션 값)\n"],"metadata":{"id":"IF5dlo1-BtlL"}},{"cell_type":"markdown","source":["### 4-3. 어텐션과 Seq2Seq\n","* 어텐션 매커니즘은 Seq2Seq 모델이 가지는 한계를 해결하기 위해 제안되었기 때문에 논문에서는 Seq2Seq 모델에 어텐션 매커니즘을 적용한 모델을 제안\n","\n","\n","```\n","Q = Query : t 시점의 디코더 셀에서의 은닉 상태\n","K = Keys : 모든 시점의 인코더 셀의 은닉 상태들\n","V = Values : 모든 시점의 인코더 셀의 은닉 상태들\n","```"],"metadata":{"id":"TlFhq5_CCUrg"}},{"cell_type":"markdown","source":["### 4-4 어텐션의 작동 원리\n","* 디코더의 첫번째, 두번째 LSTM셀은 어텐션 매커니즘을 통해 단어를 예측하는 과정을 거쳤다고 가정하고 디코더의 세번째 LSTM 셀은 출력 단어를 예측하기 위해서 딘코더의 모든 입력 단어들의 정보를 다시 참고\n","* 인코더의 시점을 각각 1, 2, ..N이라고 했을 댸 인코더의 각 셀에 대한 hidden state를 각각 h1, h2,.. hn이라고 하고, 디코더의 현재 시점 t에서 해당 셀의 hidden state를 St\n","* 어텐션 스코더는 hidden state 벡터 간 dot product를 사용하여 계산함\n","\n","![](https://wikidocs.net/images/page/22893/dotproductattention2_final.PNG)"],"metadata":{"id":"c8v0KyPFEV-f"}},{"cell_type":"markdown","source":["### 4-5. 다양한 종류의 어텐션\n","\n","<img src='https://velog.velcdn.com/images%2Fsjinu%2Fpost%2F4cf43124-67c3-4160-9478-a46567bfc5d2%2Fimage.png'>"],"metadata":{"id":"1dCY4AhNFcAJ"}}]}